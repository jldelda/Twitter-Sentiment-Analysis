---
title: "Inauguration's Day"
author: "Jose Luis Delgado Davara (@jldelda)"
date: "Sunday, January 29, 2017"
output: html_document
---

### Import libraries
```{r libraries}
library(tidytext)
library(stringr)
library(dplyr)
```

```{r Read data}
df = read.csv("sentiment2.csv")
#df = df[ ,2:30]#Remove carto id column
df = data.frame(df)
#Filter only english twits
df = df %>% filter(twitter_lang == "en") #Filter only twits in english
df$body = as.character(df$body) # Convert the tweet in text (not factor)
```

## Sentiment analysis
```{r}
review_words <- df %>%
  select(cartodb_id, body) %>%
  unnest_tokens(word, body) %>% #Tokenize the twits
  filter(!word %in% stop_words$word, 
         str_detect(word, "^[a-z']+$")) #Remove stop words, hashtags and mentions

AFINN <- sentiments %>% # Take the lexicon AFINN
  filter(lexicon == "AFINN") %>%
  select(word, afinn_score = score)

# Inner join between the tokenized tweets and the lexicon
reviews_sentiment <- review_words %>%
  inner_join(AFINN, by = "word") %>%
  group_by(cartodb_id) %>% #group and get mean afinn score by tweet 
  summarize(sentiment = mean(afinn_score))

reviews_sentiment$pos_neg <- ifelse((reviews_sentiment$sentiment > 0), 1, 
+    ifelse((reviews_sentiment$sentiment < 0), -1, 0))

## Join results with big dataset
Tweet_sentiment = merge(df, reviews_sentiment, by = 'cartodb_id')
Tweet_sentiment = Tweet_sentiment[,2:32] #Remove cartodb_id to avoid future conflicts
write.csv(Tweet_sentiment, file = "Tweet_sentiment.csv",row.names=TRUE)
```   
